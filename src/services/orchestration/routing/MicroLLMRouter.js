/**
 * ============================================================================
 * MICRO-LLM ROUTER - ORCHESTRATION ROUTING
 * ============================================================================
 * 
 * PURPOSE: Fast, cheap routing using gpt-4o-mini
 * ARCHITECTURE: OpenAI API â†’ Structured JSON output
 * PERFORMANCE: ~280ms average latency, $0.00008 per call
 * DOMAIN: Routing
 * 
 * FEATURES:
 * - Structured output enforcement (JSON mode)
 * - Retry logic with exponential backoff
 * - Fallback to keyword matching if LLM fails
 * - Timeout protection (5 second hard limit)
 * - Confidence calibration
 * 
 * USED BY: OrchestrationEngine.js (Step 5: Routing Decision)
 * 
 * @example
 * const decision = await MicroLLMRouter.route({
 *   prompt: compiledPrompt,
 *   userInput: "my AC is broken",
 *   companyId: "123",
 *   callId: "call-456"
 * });
 * // Returns: { target: "HVAC_REPAIR", thought: "AC repair keywords", confidence: 0.92 }
 * 
 * ============================================================================
 */

const logger = require('../../../../utils/logger');
const openaiClient = require('../../../../config/openai');

// ============================================================================
// CONFIGURATION
// ============================================================================

const CONFIG = {
  model: 'gpt-4o-mini',
  temperature: 0.1, // Low temp for consistent routing
  maxTokens: 150, // Small response (just JSON)
  timeout: 5000, // 5 second hard timeout
  retryAttempts: 2,
  retryDelayMs: 500
};

// ============================================================================
// MAIN CLASS
// ============================================================================

class MicroLLMRouter {
  
  /**
   * Route user input to scenario using Micro-LLM
   * 
   * @param {Object} params - Routing parameters
   * @param {string} params.prompt - Compiled prompt from CompactPromptCompiler
   * @param {string} params.userInput - User's spoken input (cleaned)
   * @param {string} params.companyId - For logging
   * @param {string} params.callId - For logging
   * @returns {Promise<Object>} Routing decision
   * @returns {string} return.target - Target scenario key
   * @returns {string} return.thought - Routing thought process
   * @returns {number} return.confidence - Confidence score 0.0-1.0
   * @returns {string} return.priority - Priority level (NORMAL, HIGH, EMERGENCY)
   * @returns {number} return.latency - Routing latency in ms
   * @returns {string} return.model - LLM model used
   * @returns {boolean} return.success - Whether routing succeeded
   * @returns {boolean} [return.fallback] - Whether fallback was used
   * 
   * @example
   * const decision = await MicroLLMRouter.route({
   *   prompt: "You are Frontline-Intel...",
   *   userInput: "my heater is broken",
   *   companyId: "507f1f77bcf86cd799439011",
   *   callId: "call_abc123"
   * });
   * // Returns: {
   * //   target: "HVAC_NO_HEAT",
   * //   thought: "Detected heating issue keywords",
   * //   confidence: 0.89,
   * //   priority: "NORMAL",
   * //   latency: 285,
   * //   model: "gpt-4o-mini",
   * //   success: true
   * // }
   */
  static async route({ prompt, userInput, companyId, callId }) {
    const startTime = Date.now();
    
    try {
      // Build user message
      const userMessage = `CALLER SAID: "${userInput}"\n\nRoute this caller to the correct scenario.`;
      
      // Attempt LLM routing with retry
      let attempt = 0;
      let lastError = null;
      
      while (attempt < CONFIG.retryAttempts) {
        try {
          const decision = await this._callLLM(prompt, userMessage, companyId, callId);
          
          // Validate decision
          if (this._isValidDecision(decision)) {
            const latency = Date.now() - startTime;
            
            logger.info('[MICRO-LLM ROUTER] Routing success', {
              companyId,
              callId,
              target: decision.target,
              confidence: decision.confidence,
              priority: decision.priority,
              latency,
              attempt: attempt + 1
            });
            
            return {
              ...decision,
              latency,
              model: CONFIG.model,
              success: true
            };
          } else {
            throw new Error('Invalid LLM response structure');
          }
          
        } catch (err) {
          lastError = err;
          attempt++;
          
          if (attempt < CONFIG.retryAttempts) {
            logger.warn('[MICRO-LLM ROUTER] Retry attempt', {
              companyId,
              callId,
              attempt,
              error: err.message
            });
            
            // Exponential backoff
            await this._sleep(CONFIG.retryDelayMs * attempt);
          }
        }
      }
      
      // All retries failed - use fallback
      logger.error('[MICRO-LLM ROUTER] All attempts failed, using fallback', {
        companyId,
        callId,
        error: lastError?.message
      });
      
      return this._fallbackRouting(userInput, prompt, companyId, callId);
      
    } catch (err) {
      logger.error('[MICRO-LLM ROUTER] Critical routing failure', {
        error: err.message,
        stack: err.stack,
        companyId,
        callId
      });
      
      // Emergency fallback
      return this._emergencyFallback(userInput);
    }
  }
  
  /**
   * Call OpenAI gpt-4o-mini
   * @private
   * @param {string} systemPrompt - System prompt
   * @param {string} userMessage - User message
   * @param {string} companyId - For logging
   * @param {string} callId - For logging
   * @returns {Promise<Object>} LLM decision
   */
  static async _callLLM(systemPrompt, userMessage, companyId, callId) {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), CONFIG.timeout);
    
    try {
      const completion = await openaiClient.chat.completions.create({
        model: CONFIG.model,
        temperature: CONFIG.temperature,
        max_tokens: CONFIG.maxTokens,
        response_format: { type: 'json_object' }, // Force JSON output
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userMessage }
        ]
      }, {
        signal: controller.signal
      });
      
      clearTimeout(timeoutId);
      
      const content = completion.choices[0].message.content;
      const decision = JSON.parse(content);
      
      // Log token usage
      logger.debug('[MICRO-LLM ROUTER] LLM call complete', {
        companyId,
        callId,
        tokensUsed: completion.usage.total_tokens,
        promptTokens: completion.usage.prompt_tokens,
        completionTokens: completion.usage.completion_tokens
      });
      
      return {
        ...decision,
        tokensUsed: completion.usage.total_tokens
      };
      
    } catch (err) {
      clearTimeout(timeoutId);
      
      if (err.name === 'AbortError') {
        throw new Error('LLM call timeout after 5 seconds');
      }
      
      throw err;
    }
  }
  
  /**
   * Validate LLM decision structure
   * @private
   * @param {Object} decision - LLM decision object
   * @returns {boolean} Whether decision is valid
   */
  static _isValidDecision(decision) {
    return (
      decision &&
      typeof decision.target === 'string' &&
      decision.target.length > 0 &&
      typeof decision.confidence === 'number' &&
      decision.confidence >= 0 &&
      decision.confidence <= 1
    );
  }
  
  /**
   * Fallback routing using keyword matching
   * @private
   * @param {string} userInput - User input
   * @param {string} prompt - Compiled prompt
   * @param {string} companyId - For logging
   * @param {string} callId - For logging
   * @returns {Object} Fallback routing decision
   */
  static _fallbackRouting(userInput, prompt, companyId, callId) {
    logger.info('[MICRO-LLM ROUTER] Using keyword fallback', {
      companyId,
      callId
    });
    
    // Extract triage rules from prompt
    const rulesMatch = prompt.match(/ROUTING RULES:\s*(\[[\s\S]*?\])/);
    if (!rulesMatch) {
      return this._emergencyFallback(userInput);
    }
    
    try {
      const rules = JSON.parse(rulesMatch[1]);
      const normalizedInput = userInput.toLowerCase();
      
      // Find best matching rule
      let bestMatch = null;
      let bestScore = 0;
      
      for (const rule of rules) {
        let score = 0;
        
        // Check positive keywords
        for (const keyword of rule.keywords || []) {
          if (normalizedInput.includes(keyword.toLowerCase())) {
            score += 1;
          }
        }
        
        // Check negative keywords (disqualify if found)
        let disqualified = false;
        for (const negKeyword of rule.negativeKeywords || []) {
          if (normalizedInput.includes(negKeyword.toLowerCase())) {
            disqualified = true;
            break;
          }
        }
        
        if (!disqualified && score > bestScore) {
          bestScore = score;
          bestMatch = rule;
        }
      }
      
      if (bestMatch && bestScore > 0) {
        return {
          target: bestMatch.scenario,
          thought: `Keyword fallback: matched ${bestScore} keywords`,
          confidence: Math.min(0.7, bestScore * 0.2), // Lower confidence for fallback
          priority: 'NORMAL',
          success: true,
          fallback: true
        };
      }
      
    } catch (err) {
      logger.error('[MICRO-LLM ROUTER] Fallback routing failed', {
        error: err.message,
        companyId,
        callId
      });
    }
    
    // Fallback to emergency
    return this._emergencyFallback(userInput);
  }
  
  /**
   * Emergency fallback (when everything fails)
   * @private
   * @param {string} userInput - User input
   * @returns {Object} Emergency fallback decision
   */
  static _emergencyFallback(userInput) {
    // Detect emergency keywords
    const emergencyWords = ['emergency', 'urgent', 'fire', 'flood', 'gas', 'smoke'];
    const isEmergency = emergencyWords.some(word => 
      userInput.toLowerCase().includes(word)
    );
    
    return {
      target: 'GENERAL_INQUIRY',
      thought: 'Emergency fallback - LLM unavailable',
      confidence: 0.3,
      priority: isEmergency ? 'EMERGENCY' : 'NORMAL',
      success: true,
      emergency: true,
      fallback: true
    };
  }
  
  /**
   * Sleep utility for retry backoff
   * @private
   * @param {number} ms - Milliseconds to sleep
   * @returns {Promise<void>}
   */
  static _sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
  
  /**
   * Batch route multiple inputs (for testing/analysis)
   * 
   * @param {Array<Object>} inputs - Array of { prompt, userInput, companyId, callId }
   * @returns {Promise<Array<Object>>} Array of routing decisions
   * 
   * @example
   * const results = await MicroLLMRouter.routeBatch([
   *   { prompt: "...", userInput: "AC broken", companyId: "123", callId: "1" },
   *   { prompt: "...", userInput: "need quote", companyId: "123", callId: "2" }
   * ]);
   * // Returns array of routing decisions
   */
  static async routeBatch(inputs) {
    const results = [];
    
    for (const input of inputs) {
      try {
        const decision = await this.route(input);
        results.push(decision);
      } catch (err) {
        results.push({
          error: err.message,
          input: input.userInput,
          success: false
        });
      }
    }
    
    return results;
  }
}

// ============================================================================
// EXPORTS
// ============================================================================

module.exports = MicroLLMRouter;

