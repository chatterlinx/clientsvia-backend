/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * FRONTLINE-INTEL SERVICE
 * The Intelligent Gatekeeper - Command Layer for Call Processing
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 * Purpose: Acts as the AI receptionist that processes EVERY call before routing
 * 
 * Capabilities:
 * 1. Intent Extraction - Understands messy, rambling caller input
 * 2. Customer Recognition - Looks up returning customers in database
 * 3. Service Validation - Detects wrong company or wrong service
 * 4. Context Capture - Remembers story details for human-like responses
 * 5. Input Normalization - Cleans input for Tier 1/2/3 routing
 * 6. Short-Circuit Detection - Politely redirects wrong numbers/services
 * 
 * Design Philosophy:
 * - Quality over speed (Phase 1: every call uses Frontline-Intel)
 * - Graceful degradation (fallback to raw input if LLM fails)
 * - Rich telemetry (track performance, costs, success rates)
 * - Future-proof (supports fast-path optimization in Phase 2)
 * 
 * Performance:
 * - Avg time: ~800ms (LLM call)
 * - Avg cost: $0.001-0.003 per call
 * - Success rate: 98%+ (with fallback handling)
 * 
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

const openai = require('../config/openai');
const logger = require('../utils/logger');

// Lazy-load to avoid circular dependencies
let Contact;
const getModels = () => {
    if (!Contact) Contact = require('../models/v2Contact');
    return { Contact };
};

class FrontlineIntel {
    /**
     * Process caller input through Frontline-Intel
     * 
     * @param {string} userInput - Raw caller speech (from Twilio)
     * @param {Object} company - Company document with cheatSheet
     * @param {string} callerPhone - Caller's phone number
     * @param {Object} options - Processing options
     * @returns {Promise<Object>} Frontline-Intel output
     */
    static async run(userInput, company, callerPhone, options = {}) {
        const startTime = Date.now();
        const config = company?.aiAgentSettings?.callFlowConfig?.find(s => s.id === 'frontlineIntel');
        
        if (!config || !config.enabled) {
            logger.info('ğŸ¯ [FRONTLINE-INTEL] Disabled, using raw input');
            return {
                skipped: true,
                cleanedInput: userInput,
                customer: null,
                callValidation: { correctCompany: true, correctService: true },
                shouldShortCircuit: false,
                context: null,
                confidence: 1.0,
                timeMs: 0,
                cost: 0
            };
        }
        
        try {
            logger.info('ğŸ§  [FRONTLINE-INTEL] Processing call...');
            logger.info(`ğŸ“ Input: "${userInput.substring(0, 100)}${userInput.length > 100 ? '...' : ''}"`);
            
            // Step 1: Check fast-path (Phase 2 feature, disabled by default)
            if (config.params?.fastPath?.enabled) {
                const fastPathResult = this.checkFastPath(userInput, config.params.fastPath);
                if (fastPathResult) {
                    logger.info('ğŸš€ [FRONTLINE-INTEL] Fast-path hit, skipping LLM');
                    return {
                        fastPath: true,
                        ...fastPathResult,
                        timeMs: Date.now() - startTime,
                        cost: 0
                    };
                }
            }
            
            // Step 2: Build system prompt with company-specific instructions
            const systemPrompt = this.buildSystemPrompt(company);
            
            // Step 3: Call OpenAI with timeout and retry logic
            const llmResult = await this.callLLMWithRetry(
                systemPrompt,
                userInput,
                config.params || {},
                startTime
            );
            
            // Step 4: Customer lookup (if name or phone extracted)
            if (config.params?.enableCustomerLookup && llmResult.customer?.shouldLookup) {
                logger.info('ğŸ” [FRONTLINE-INTEL] Looking up customer...');
                const customerInfo = await this.lookupCustomer({
                    name: llmResult.customer.name,
                    phone: llmResult.customer.phone || callerPhone,
                    companyId: company._id
                });
                
                if (customerInfo) {
                    logger.info(`âœ… [FRONTLINE-INTEL] Found returning customer: ${customerInfo.name}`);
                    llmResult.customer = {
                        ...llmResult.customer,
                        ...customerInfo,
                        isExisting: true
                    };
                } else {
                    logger.info('ğŸ“ [FRONTLINE-INTEL] New customer (not in database)');
                    llmResult.customer.isExisting = false;
                }
            }
            
            // Step 5: Calculate telemetry
            const timeMs = Date.now() - startTime;
            const cost = this.calculateCost(llmResult.tokens, config.params?.model || 'gpt-4o-mini');
            
            logger.info(`âœ… [FRONTLINE-INTEL] Complete in ${timeMs}ms, cost: $${cost.toFixed(4)}`);
            logger.info(`ğŸ“Š Intent: ${llmResult.detectedIntent}, Confidence: ${llmResult.confidence}`);
            
            if (llmResult.shouldShortCircuit) {
                logger.warn(`âš ï¸ [FRONTLINE-INTEL] Short-circuit detected: ${llmResult.callValidation?.reasoning}`);
            }
            
            return {
                ...llmResult,
                timeMs,
                cost,
                model: config.params?.model || 'gpt-4o-mini'
            };
            
        } catch (error) {
            logger.error('âŒ [FRONTLINE-INTEL] Error processing call:', error);
            
            // Graceful degradation: fallback to raw input
            if (config.params?.fallbackToRaw) {
                logger.warn('ğŸ”„ [FRONTLINE-INTEL] Falling back to raw input');
                return {
                    error: true,
                    cleanedInput: userInput,
                    customer: null,
                    callValidation: { correctCompany: true, correctService: true },
                    shouldShortCircuit: false,
                    context: null,
                    confidence: 0.5,
                    timeMs: Date.now() - startTime,
                    cost: 0
                };
            }
            
            throw error;
        }
    }
    
    /**
     * Build system prompt for Frontline-Intel LLM
     */
    static buildSystemPrompt(company) {
        const companyName = company?.businessName || company?.companyName || 'the company';
        const services = company?.services || ['HVAC services'];
        const frontlineIntel = company?.aiAgentSettings?.cheatSheet?.frontlineIntel || '';
        
        return `You are the Frontline-Intel for ${companyName}.

YOUR ROLE:
You are the FIRST INTELLIGENT LAYER that processes every call.
You act as the company's front desk intelligence - smart, observant, and decisive.

YOUR RESPONSIBILITIES:
1. Listen carefully to the caller (even if they ramble)
2. Extract their name and contact information
3. Identify what they need (intent)
4. Validate this is the right company and service
5. Look up customer information if they mention a name
6. Decide: Should we handle this call, or redirect politely?

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPANY INFORMATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Company Name: ${companyName}
Services We Provide: ${Array.isArray(services) ? services.join(', ') : services}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPANY-SPECIFIC PROTOCOLS (Frontline-Intel Instructions)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

${frontlineIntel || 'No specific instructions provided. Use general best practices.'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CALL VALIDATION RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. WRONG COMPANY DETECTION:
   - If caller asks "Is this [other company name]?"
   - If caller mentions a different company repeatedly
   â†’ Set correctCompany: false
   â†’ Provide polite redirect in shortCircuitResponse

2. WRONG SERVICE DETECTION:
   - If caller needs a service we DON'T provide
   - Check against our services list above
   â†’ Set correctService: false
   â†’ Provide helpful redirect in shortCircuitResponse

3. CUSTOMER IDENTIFICATION:
   - If caller says "This is [name]" or "My name is [name]"
   - Extract: name, phone number (if mentioned)
   - Set shouldLookup: true for database search

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OUTPUT FORMAT (JSON ONLY - NO OTHER TEXT)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Respond ONLY with valid JSON in this exact format:

{
  "cleanedInput": "Brief summary of their need",
  "detectedIntent": "repair | maintenance | billing | emergency | question | wrong_number | wrong_service | other",
  "keywords": ["keyword1", "keyword2", "keyword3"],
  "urgency": "emergency | urgent | normal | low",
  "tone": "friendly | upset | confused | angry | neutral",
  
  "customer": {
    "name": "extracted name or null",
    "phone": "extracted phone or null",
    "shouldLookup": true or false
  },
  
  "callValidation": {
    "correctCompany": true or false,
    "correctService": true or false,
    "reasoning": "why wrong company/service, or null"
  },
  
  "shouldShortCircuit": true or false,
  "shortCircuitResponse": "polite redirect message or null",
  
  "entities": {
    "problem": "what's broken or needed",
    "system": "AC, heater, plumbing, etc."
  },
  
  "context": {
    "customerStory": "brief summary of their story",
    "impossibleRequests": "any impossible requests or null",
    "shouldHandle": "how to handle special situations"
  },
  
  "suggestedScenario": "scenario_name or null",
  "confidence": 0.0 to 1.0
}

CRITICAL: Return ONLY the JSON object, no other text before or after!`;
    }
    
    /**
     * Call OpenAI LLM with retry logic
     */
    static async callLLMWithRetry(systemPrompt, userInput, params, startTime) {
        const model = params.model || 'gpt-4o-mini';
        const timeout = params.timeout || 5000;
        const retries = params.retries || 1;
        const maxCost = params.maxCostPerCall || 0.01;
        
        for (let attempt = 1; attempt <= retries + 1; attempt++) {
            try {
                // Check timeout
                if (Date.now() - startTime > timeout) {
                    throw new Error(`Timeout exceeded (${timeout}ms)`);
                }
                
                logger.info(`ğŸ¤– [FRONTLINE-INTEL] LLM call (attempt ${attempt}/${retries + 1}), model: ${model}`);
                
                const completion = await openai.chat.completions.create({
                    model: model,
                    messages: [
                        { role: 'system', content: systemPrompt },
                        { role: 'user', content: userInput }
                    ],
                    response_format: { type: 'json_object' },
                    temperature: 0.3,  // Low temperature for consistent parsing
                    max_tokens: 1000   // Plenty for structured output
                });
                
                const rawResponse = completion.choices[0].message.content;
                const result = JSON.parse(rawResponse);
                
                // Add token usage for cost calculation
                result.tokens = completion.usage;
                
                // Validate cost doesn't exceed limit
                const estimatedCost = this.calculateCost(result.tokens, model);
                if (estimatedCost > maxCost) {
                    logger.warn(`âš ï¸ [FRONTLINE-INTEL] Cost $${estimatedCost.toFixed(4)} exceeds limit $${maxCost}`);
                }
                
                return result;
                
            } catch (error) {
                logger.error(`âŒ [FRONTLINE-INTEL] LLM call failed (attempt ${attempt}):`, error.message);
                
                if (attempt >= retries + 1) {
                    throw error;
                }
                
                // Wait before retry (exponential backoff)
                await new Promise(resolve => setTimeout(resolve, 500 * attempt));
            }
        }
    }
    
    /**
     * Look up customer in database
     */
    static async lookupCustomer({ name, phone, companyId }) {
        try {
            const { Contact } = getModels();
            
            // Build query (fuzzy name match OR exact phone match)
            const query = {
                companyId: companyId,
                $or: []
            };
            
            if (name) {
                // Search in both fullName and firstName/lastName
                query.$or.push({ fullName: new RegExp(name, 'i') });
                query.$or.push({ firstName: new RegExp(name, 'i') });
                query.$or.push({ lastName: new RegExp(name, 'i') });
            }
            if (phone) {
                // Normalize phone (remove formatting)
                const normalizedPhone = phone.replace(/\D/g, '');
                query.$or.push({ primaryPhone: new RegExp(normalizedPhone) });
                query.$or.push({ alternatePhone: new RegExp(normalizedPhone) });
            }
            
            if (query.$or.length === 0) {
                return null;  // No search criteria
            }
            
            // Find contact (most recent first)
            const contact = await Contact.findOne(query)
                .sort({ lastContactDate: -1 })
                .lean();
            
            if (!contact) {
                return null;
            }
            
            // Extract recent service history from interactions
            const recentServices = (contact.interactions || [])
                .filter(i => i.type === 'call' || i.type === 'appointment')
                .sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp))
                .slice(0, 3)
                .map(interaction => ({
                    date: interaction.timestamp,
                    issue: interaction.summary || 'No summary',
                    outcome: interaction.outcome
                }));
            
            return {
                customerId: contact._id.toString(),
                name: contact.fullName || `${contact.firstName || ''} ${contact.lastName || ''}`.trim(),
                phone: contact.primaryPhone,
                alternatePhone: contact.alternatePhone,
                email: contact.email,
                lastContactDate: contact.lastContactDate,
                status: contact.status,
                customerType: contact.customerType,
                totalCalls: contact.totalCalls || 0,
                recentServices,
                serviceRequests: (contact.serviceRequests || []).slice(0, 3).map(sr => ({
                    serviceType: sr.serviceType,
                    status: sr.status,
                    requestedDate: sr.requestedDate
                }))
            };
            
        } catch (error) {
            logger.error('âŒ [FRONTLINE-INTEL] Customer lookup failed:', error);
            return null;  // Non-critical error, continue without customer info
        }
    }
    
    /**
     * Check fast-path patterns (Phase 2 feature)
     */
    static checkFastPath(userInput, fastPathConfig) {
        if (!fastPathConfig || !fastPathConfig.enabled || !fastPathConfig.patterns) {
            return null;
        }
        
        const input = userInput.toLowerCase().trim();
        
        for (const pattern of fastPathConfig.patterns) {
            const regex = new RegExp(pattern.pattern, 'i');
            if (regex.test(input)) {
                return {
                    cleanedInput: input,
                    fastPathPattern: pattern.pattern,
                    fastPathConfidence: pattern.confidence || 0.95,
                    shouldSkipTiers: fastPathConfig.skipFrontlineIntel,
                    routeTo: fastPathConfig.routeDirectlyTo || 'scenarioMatching'
                };
            }
        }
        
        return null;
    }
    
    /**
     * Calculate LLM cost based on token usage
     */
    static calculateCost(tokens, model) {
        if (!tokens) return 0;
        
        // Pricing per 1M tokens (as of 2024)
        const pricing = {
            'gpt-4o-mini': { input: 0.150, output: 0.600 },    // $0.15/$0.60 per 1M
            'gpt-4o': { input: 2.50, output: 10.00 },          // $2.50/$10 per 1M
            'gpt-4-turbo': { input: 10.00, output: 30.00 }     // $10/$30 per 1M
        };
        
        const rates = pricing[model] || pricing['gpt-4o-mini'];
        
        const inputCost = (tokens.prompt_tokens / 1000000) * rates.input;
        const outputCost = (tokens.completion_tokens / 1000000) * rates.output;
        
        return inputCost + outputCost;
    }
}

module.exports = FrontlineIntel;

