/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * FRONTLINE-INTEL SERVICE
 * The Intelligent Gatekeeper - Command Layer for Call Processing
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 * Purpose: Acts as the AI receptionist that processes EVERY call before routing
 * 
 * Capabilities:
 * 1. Intent Extraction - Understands messy, rambling caller input
 * 2. Customer Recognition - Looks up returning customers in database
 * 3. Service Validation - Detects wrong company or wrong service
 * 4. Context Capture - Remembers story details for human-like responses
 * 5. Input Normalization - Cleans input for Tier 1/2/3 routing
 * 6. Short-Circuit Detection - Politely redirects wrong numbers/services
 * 
 * Design Philosophy:
 * - Quality over speed (Phase 1: every call uses Frontline-Intel)
 * - Graceful degradation (fallback to raw input if LLM fails)
 * - Rich telemetry (track performance, costs, success rates)
 * - Future-proof (supports fast-path optimization in Phase 2)
 * 
 * Performance:
 * - Avg time: ~800ms (LLM call)
 * - Avg cost: $0.001-0.003 per call
 * - Success rate: 98%+ (with fallback handling)
 * 
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

const openai = require('../config/openai');
const logger = require('../utils/logger');
const { replacePlaceholders } = require('../utils/placeholderReplacer');
const TriageCardService = require('./TriageCardService');

// V23: Import the fixed template and scenario loader
const { buildFrontlinePromptV23, buildMicroLLMPrompt } = require('../templates/FrontlineSystemPromptV23');
const { getActiveScenariosForCompany } = require('./ActiveScenariosHelper');

// Lazy-load to avoid circular dependencies
let Contact;
let TriageCard;
const getModels = () => {
    if (!Contact) Contact = require('../models/v2Contact');
    if (!TriageCard) TriageCard = require('../models/TriageCard');
    return { Contact, TriageCard };
};

class FrontlineIntel {
    /**
     * Process caller input through Frontline-Intel
     * 
     * @param {string} userInput - Raw caller speech (from Twilio)
     * @param {Object} company - Company document with cheatSheet
     * @param {string} callerPhone - Caller's phone number
     * @param {Object} options - Processing options
     * @returns {Promise<Object>} Frontline-Intel output
     */
    static async run(userInput, company, callerPhone, options = {}) {
        const startTime = Date.now();
        const config = company?.aiAgentSettings?.callFlowConfig?.find(s => s.id === 'frontlineIntel');
        
        if (!config || !config.enabled) {
            logger.info('ðŸŽ¯ [FRONTLINE-INTEL] Disabled, using raw input');
            return {
                skipped: true,
                cleanedInput: userInput,
                customer: null,
                callValidation: { correctCompany: true, correctService: true },
                shouldShortCircuit: false,
                context: null,
                confidence: 1.0,
                timeMs: 0,
                cost: 0
            };
        }
        
        try {
            logger.info('ðŸ§  [FRONTLINE-INTEL] Processing call...');
            logger.info(`ðŸ“ž Input: "${userInput.substring(0, 100)}${userInput.length > 100 ? '...' : ''}"`);
            
            // Step 1: Check fast-path (Phase 2 feature, disabled by default)
            if (config.params?.fastPath?.enabled) {
                const fastPathResult = this.checkFastPath(userInput, config.params.fastPath);
                if (fastPathResult) {
                    logger.info('ðŸš€ [FRONTLINE-INTEL] Fast-path hit, skipping LLM');
                    return {
                        fastPath: true,
                        ...fastPathResult,
                        timeMs: Date.now() - startTime,
                        cost: 0
                    };
                }
            }
            
            // Step 2: Build system prompt with company-specific instructions
            // V23: Check if V23 mode is enabled for this company
            const useV23 = config.params?.useV23Template || company?.aiAgentSettings?.useV23FrontlineTemplate || false;
            
            let systemPrompt;
            let v23PromptData = null;
            
            if (useV23) {
                // V23: Load triage rules and scenarios for dynamic injection
                logger.info('ðŸš€ [FRONTLINE-INTEL] V23 mode enabled - loading dynamic prompt data...');
                v23PromptData = await this.loadV23PromptData(company._id);
                
                systemPrompt = this.buildSystemPrompt(company, {
                    useV23Template: true,
                    triageRules: v23PromptData.triageRules,
                    activeScenarios: v23PromptData.activeScenarios,
                    useMicroPrompt: config.params?.useMicroPrompt || false
                });
                
                logger.info(`ðŸš€ [FRONTLINE-INTEL V23] Prompt assembled with ${v23PromptData.triageRules.length} rules, ${v23PromptData.activeScenarios.length} scenarios`);
            } else {
                // Legacy mode
                systemPrompt = this.buildSystemPrompt(company);
            }
            
            // Step 3: Call OpenAI with timeout and retry logic
            const llmResult = await this.callLLMWithRetry(
                systemPrompt,
                userInput,
                config.params || {},
                startTime
            );
            
            // Step 4: Customer lookup (if name or phone extracted)
            if (config.params?.enableCustomerLookup && llmResult.customer?.shouldLookup) {
                logger.info('ðŸ” [FRONTLINE-INTEL] Looking up customer...');
                const customerInfo = await this.lookupCustomer({
                    name: llmResult.customer.name,
                    phone: llmResult.customer.phone || callerPhone,
                    companyId: company._id
                });
                
                if (customerInfo) {
                    logger.info(`âœ… [FRONTLINE-INTEL] Found returning customer: ${customerInfo.name}`);
                    llmResult.customer = {
                        ...llmResult.customer,
                        ...customerInfo,
                        isExisting: true
                    };
                } else {
                    logger.info('ðŸ“ [FRONTLINE-INTEL] New customer (not in database)');
                    llmResult.customer.isExisting = false;
                }
            }
            
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // ðŸ§  THE BRAIN: Triage Engine - Determine service type & routing action
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            // This is where we classify the call and decide what to do with it.
            // Loads ONE unified triage brain (manual rules + AI cards + fallback)
            // First match wins by priority.
            // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            let triageDecision = null;
            
            try {
                logger.info('ðŸ§  [THE BRAIN] Loading compiled triage config...');
                const compiledConfig = await TriageCardService.compileActiveCards(company._id);
                
                logger.info(`ðŸ§  [THE BRAIN] Loaded ${compiledConfig.triageRules?.length || 0} rules (manual + AI + fallback)`);
                
                // Match caller input against triage rules (first match wins)
                triageDecision = this.matchTriageRules(
                    userInput,
                    compiledConfig.triageRules || [],
                    {
                        llmKeywords: llmResult.keywords || [],
                        llmIntent: llmResult.detectedIntent
                    }
                );
                
                if (triageDecision) {
                    logger.info('ðŸ§  [THE BRAIN] Triage decision made', {
                        source: triageDecision.source,
                        priority: triageDecision.priority,
                        keywords: triageDecision.keywords,
                        serviceType: triageDecision.serviceType,
                        action: triageDecision.action,
                        categorySlug: triageDecision.categorySlug
                    });
                } else {
                    logger.warn('ðŸ§  [THE BRAIN] No triage rule matched (should not happen with fallback)');
                }
                
            } catch (triageError) {
                logger.error('âŒ [THE BRAIN] Triage matching failed:', triageError.message);
                // Non-critical: continue without triage decision
                // The 3-Tier system can still handle the call
            }
            
            // Step 5: Calculate telemetry
            const timeMs = Date.now() - startTime;
            const cost = this.calculateCost(llmResult.tokens, config.params?.model || 'gpt-4o-mini');
            
            logger.info(`âœ… [FRONTLINE-INTEL] Complete in ${timeMs}ms, cost: $${cost.toFixed(4)}`);
            logger.info(`ðŸ“Š Intent: ${llmResult.detectedIntent}, Confidence: ${llmResult.confidence}`);
            
            if (llmResult.shouldShortCircuit) {
                logger.warn(`âš ï¸ [FRONTLINE-INTEL] Short-circuit detected: ${llmResult.callValidation?.reasoning}`);
            }
            
            return {
                ...llmResult,
                triageDecision,  // ðŸ§  THE BRAIN's decision
                timeMs,
                cost,
                model: config.params?.model || 'gpt-4o-mini',
                // V23 metadata
                v23Mode: useV23,
                v23PromptData: useV23 ? {
                    triageRulesCount: v23PromptData?.triageRules?.length || 0,
                    scenariosCount: v23PromptData?.activeScenarios?.length || 0,
                    loadTimeMs: v23PromptData?.loadTimeMs || 0
                } : null
            };
            
        } catch (error) {
            logger.error('âŒ [FRONTLINE-INTEL] Error processing call:', error);
            
            // Graceful degradation: fallback to raw input
            if (config.params?.fallbackToRaw) {
                logger.warn('ðŸ”„ [FRONTLINE-INTEL] Falling back to raw input');
                return {
                    error: true,
                    cleanedInput: userInput,
                    customer: null,
                    callValidation: { correctCompany: true, correctService: true },
                    shouldShortCircuit: false,
                    context: null,
                    confidence: 0.5,
                    timeMs: Date.now() - startTime,
                    cost: 0
                };
            }
            
            throw error;
        }
    }
    
    /**
     * Build system prompt for Frontline-Intel LLM
     * V23: Supports both legacy prompt and V23 fixed template with dynamic injection
     * 
     * @param {Object} company - Company document
     * @param {Object} options - Options for prompt building
     * @param {boolean} options.useV23Template - Use V23 fixed template (default: false for backward compat)
     * @param {Array} options.triageRules - Compiled triage rules for V23 injection
     * @param {Array} options.activeScenarios - Active scenarios for V23 injection
     */
    static buildSystemPrompt(company, options = {}) {
        const companyName = company?.businessName || company?.companyName || 'the company';
        const services = company?.services || ['HVAC services'];
        const trade = company?.trade || 'SERVICE';
        
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // V23: Use fixed template with dynamic injection
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if (options.useV23Template && options.triageRules && options.activeScenarios) {
            logger.info('ðŸš€ [FRONTLINE-INTEL V23] Using fixed template with dynamic injection');
            logger.info(`   â†’ Injecting ${options.triageRules.length} triage rules`);
            logger.info(`   â†’ Injecting ${options.activeScenarios.length} active scenarios`);
            
            // Use the micro-LLM prompt for speed, or full prompt for accuracy
            const useMicroPrompt = options.useMicroPrompt || false;
            
            if (useMicroPrompt) {
                return buildMicroLLMPrompt({
                    companyName,
                    triageRules: options.triageRules,
                    activeScenarios: options.activeScenarios
                });
            }
            
            return buildFrontlinePromptV23({
                companyName,
                trade,
                serviceAreas: company?.serviceAreas || [],
                currentTime: new Date().toLocaleString(),
                triageRules: options.triageRules,
                activeScenarios: options.activeScenarios
            });
        }
        
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // LEGACY: Variable replacement for backward compatibility
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        let frontlineIntel = company?.aiAgentSettings?.cheatSheet?.frontlineIntel || '';
        
        if (frontlineIntel) {
            logger.info('ðŸ”„ [FRONTLINE-INTEL] Replacing variables in Frontline-Intel text...');
            const originalLength = frontlineIntel.length;
            frontlineIntel = replacePlaceholders(frontlineIntel, company);
            logger.info(`âœ… [FRONTLINE-INTEL] Variables replaced: ${originalLength} â†’ ${frontlineIntel.length} chars`);
        }
        
        return `You are the Frontline-Intel for ${companyName}.

YOUR ROLE:
You are the FIRST INTELLIGENT LAYER that processes every call.
You act as the company's front desk intelligence - smart, observant, and decisive.

YOUR RESPONSIBILITIES:
1. Listen carefully to the caller (even if they ramble)
2. Extract their name and contact information
3. Identify what they need (intent)
4. Validate this is the right company and service
5. Look up customer information if they mention a name
6. Decide: Should we handle this call, or redirect politely?

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPANY INFORMATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Company Name: ${companyName}
Services We Provide: ${Array.isArray(services) ? services.join(', ') : services}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPANY-SPECIFIC PROTOCOLS (Frontline-Intel Instructions)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

${frontlineIntel || 'No specific instructions provided. Use general best practices.'}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CALL VALIDATION RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. WRONG COMPANY DETECTION:
   - If caller asks "Is this [other company name]?"
   - If caller mentions a different company repeatedly
   â†’ Set correctCompany: false
   â†’ Provide polite redirect in shortCircuitResponse

2. WRONG SERVICE DETECTION:
   - If caller needs a service we DON'T provide
   - Check against our services list above
   â†’ Set correctService: false
   â†’ Provide helpful redirect in shortCircuitResponse

3. CUSTOMER IDENTIFICATION:
   - If caller says "This is [name]" or "My name is [name]"
   - Extract: name, phone number (if mentioned)
   - Set shouldLookup: true for database search

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OUTPUT FORMAT (JSON ONLY - NO OTHER TEXT)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Respond ONLY with valid JSON in this exact format:

{
  "cleanedInput": "Brief summary of their need",
  "detectedIntent": "repair | maintenance | billing | emergency | question | wrong_number | wrong_service | other",
  "keywords": ["keyword1", "keyword2", "keyword3"],
  "urgency": "emergency | urgent | normal | low",
  "tone": "friendly | upset | confused | angry | neutral",
  
  "customer": {
    "name": "extracted name or null",
    "phone": "extracted phone or null",
    "shouldLookup": true or false
  },
  
  "callValidation": {
    "correctCompany": true or false,
    "correctService": true or false,
    "reasoning": "why wrong company/service, or null"
  },
  
  "shouldShortCircuit": true or false,
  "shortCircuitResponse": "polite redirect message or null",
  
  "entities": {
    "problem": "what's broken or needed",
    "system": "AC, heater, plumbing, etc."
  },
  
  "context": {
    "customerStory": "brief summary of their story",
    "impossibleRequests": "any impossible requests or null",
    "shouldHandle": "how to handle special situations"
  },
  
  "suggestedScenario": "scenario_name or null",
  "confidence": 0.0 to 1.0
}

CRITICAL: Return ONLY the JSON object, no other text before or after!`;
    }
    
    /**
     * V23: Load triage rules and scenarios for dynamic prompt injection
     * Called once per call, results cached in call context
     */
    static async loadV23PromptData(companyId) {
        const startTime = Date.now();
        
        try {
            const { TriageCard } = getModels();
            
            // Load active triage cards
            const triageCards = await TriageCard.find({
                companyId,
                active: true
            }).sort({ priority: -1 }).lean();
            
            // Convert to prompt-friendly format
            const triageRules = triageCards.map(card => ({
                label: card.displayName || card.triageLabel,
                mustHaveKeywords: card.quickRuleConfig?.mustHaveKeywords || [],
                excludeKeywords: card.quickRuleConfig?.excludeKeywords || [],
                action: card.quickRuleConfig?.action || 'DIRECT_TO_3TIER',
                targetScenarioKey: card.threeTierLink?.scenarioKey || null,
                priority: card.priority || 100
            }));
            
            // Load active scenarios
            const scenariosResult = await getActiveScenariosForCompany(companyId);
            const activeScenarios = scenariosResult.scenarios || [];
            
            const elapsed = Date.now() - startTime;
            logger.info(`[FRONTLINE-INTEL V23] Loaded prompt data in ${elapsed}ms`, {
                triageRules: triageRules.length,
                scenarios: activeScenarios.length
            });
            
            return {
                triageRules,
                activeScenarios,
                loadTimeMs: elapsed
            };
            
        } catch (error) {
            logger.error('[FRONTLINE-INTEL V23] Failed to load prompt data', {
                error: error.message,
                companyId
            });
            return {
                triageRules: [],
                activeScenarios: [],
                loadTimeMs: Date.now() - startTime,
                error: error.message
            };
        }
    }
    
    /**
     * Call OpenAI LLM with retry logic
     */
    static async callLLMWithRetry(systemPrompt, userInput, params, startTime) {
        // Check if OpenAI client is available
        if (!openai) {
            throw new Error('OpenAI client not initialized - check OPENAI_API_KEY environment variable');
        }
        
        const model = params.model || 'gpt-4o-mini';
        const timeout = params.timeout || 5000;
        const retries = params.retries || 1;
        const maxCost = params.maxCostPerCall || 0.01;
        
        for (let attempt = 1; attempt <= retries + 1; attempt++) {
            try {
                // Check timeout
                if (Date.now() - startTime > timeout) {
                    throw new Error(`Timeout exceeded (${timeout}ms)`);
                }
                
                logger.info(`ðŸ¤– [FRONTLINE-INTEL] LLM call (attempt ${attempt}/${retries + 1}), model: ${model}`);
                
                const completion = await openai.chat.completions.create({
                    model: model,
                    messages: [
                        { role: 'system', content: systemPrompt },
                        { role: 'user', content: userInput }
                    ],
                    response_format: { type: 'json_object' },
                    temperature: 0.3,  // Low temperature for consistent parsing
                    max_tokens: 1000   // Plenty for structured output
                });
                
                const rawResponse = completion.choices[0].message.content;
                const result = JSON.parse(rawResponse);
                
                // Add token usage for cost calculation
                result.tokens = completion.usage;
                
                // Validate cost doesn't exceed limit
                const estimatedCost = this.calculateCost(result.tokens, model);
                if (estimatedCost > maxCost) {
                    logger.warn(`âš ï¸ [FRONTLINE-INTEL] Cost $${estimatedCost.toFixed(4)} exceeds limit $${maxCost}`);
                }
                
                return result;
                
            } catch (error) {
                logger.error(`âŒ [FRONTLINE-INTEL] LLM call failed (attempt ${attempt}):`, error.message);
                
                if (attempt >= retries + 1) {
                    throw error;
                }
                
                // Wait before retry (exponential backoff)
                await new Promise(resolve => setTimeout(resolve, 500 * attempt));
            }
        }
    }
    
    /**
     * Look up customer in database
     */
    static async lookupCustomer({ name, phone, companyId }) {
        try {
            const { Contact } = getModels();
            
            // Build query (fuzzy name match OR exact phone match)
            const query = {
                companyId: companyId,
                $or: []
            };
            
            if (name) {
                // Search in both fullName and firstName/lastName
                query.$or.push({ fullName: new RegExp(name, 'i') });
                query.$or.push({ firstName: new RegExp(name, 'i') });
                query.$or.push({ lastName: new RegExp(name, 'i') });
            }
            if (phone) {
                // Normalize phone (remove formatting)
                const normalizedPhone = phone.replace(/\D/g, '');
                query.$or.push({ primaryPhone: new RegExp(normalizedPhone) });
                query.$or.push({ alternatePhone: new RegExp(normalizedPhone) });
            }
            
            if (query.$or.length === 0) {
                return null;  // No search criteria
            }
            
            // Find contact (most recent first)
            const contact = await Contact.findOne(query)
                .sort({ lastContactDate: -1 })
                .lean();
            
            if (!contact) {
                return null;
            }
            
            // Extract recent service history from interactions
            const recentServices = (contact.interactions || [])
                .filter(i => i.type === 'call' || i.type === 'appointment')
                .sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp))
                .slice(0, 3)
                .map(interaction => ({
                    date: interaction.timestamp,
                    issue: interaction.summary || 'No summary',
                    outcome: interaction.outcome
                }));
            
            return {
                customerId: contact._id.toString(),
                name: contact.fullName || `${contact.firstName || ''} ${contact.lastName || ''}`.trim(),
                phone: contact.primaryPhone,
                alternatePhone: contact.alternatePhone,
                email: contact.email,
                lastContactDate: contact.lastContactDate,
                status: contact.status,
                customerType: contact.customerType,
                totalCalls: contact.totalCalls || 0,
                recentServices,
                serviceRequests: (contact.serviceRequests || []).slice(0, 3).map(sr => ({
                    serviceType: sr.serviceType,
                    status: sr.status,
                    requestedDate: sr.requestedDate
                }))
            };
            
        } catch (error) {
            logger.error('âŒ [FRONTLINE-INTEL] Customer lookup failed:', error);
            return null;  // Non-critical error, continue without customer info
        }
    }
    
    /**
     * Check fast-path patterns (Phase 2 feature)
     */
    static checkFastPath(userInput, fastPathConfig) {
        if (!fastPathConfig || !fastPathConfig.enabled || !fastPathConfig.patterns) {
            return null;
        }
        
        const input = userInput.toLowerCase().trim();
        
        for (const pattern of fastPathConfig.patterns) {
            const regex = new RegExp(pattern.pattern, 'i');
            if (regex.test(input)) {
                return {
                    cleanedInput: input,
                    fastPathPattern: pattern.pattern,
                    fastPathConfidence: pattern.confidence || 0.95,
                    shouldSkipTiers: fastPathConfig.skipFrontlineIntel,
                    routeTo: fastPathConfig.routeDirectlyTo || 'scenarioMatching'
                };
            }
        }
        
        return null;
    }
    
    /**
     * Calculate LLM cost based on token usage
     */
    static calculateCost(tokens, model) {
        if (!tokens) return 0;
        
        // Pricing per 1M tokens (as of 2024)
        const pricing = {
            'gpt-4o-mini': { input: 0.150, output: 0.600 },    // $0.15/$0.60 per 1M
            'gpt-4o': { input: 2.50, output: 10.00 },          // $2.50/$10 per 1M
            'gpt-4-turbo': { input: 10.00, output: 30.00 }     // $10/$30 per 1M
        };
        
        const rates = pricing[model] || pricing['gpt-4o-mini'];
        
        const inputCost = (tokens.prompt_tokens / 1000000) * rates.input;
        const outputCost = (tokens.completion_tokens / 1000000) * rates.output;
        
        return inputCost + outputCost;
    }
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ðŸ§  THE BRAIN: Triage Matching Engine
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    /**
     * Match caller input against triage rules (ONE LOOP, FIRST MATCH WINS)
     * @param {string} userInput - Raw caller speech
     * @param {Array} triageRules - Compiled rules (manual + AI cards + fallback, sorted by priority)
     * @param {Object} context - Additional context from LLM (keywords, intent)
     * @returns {Object|null} Matched rule or null
     */
    static matchTriageRules(userInput, triageRules, context = {}) {
        if (!triageRules || triageRules.length === 0) {
            logger.warn('ðŸ§  [THE BRAIN] No triage rules provided');
            return null;
        }
        
        const input = userInput.toLowerCase().trim();
        const llmKeywords = (context.llmKeywords || []).map(k => k.toLowerCase());
        
        logger.info(`ðŸ§  [THE BRAIN] Matching against ${triageRules.length} rules...`);
        logger.info(`ðŸ§  [THE BRAIN] Input: "${input.substring(0, 100)}${input.length > 100 ? '...' : ''}"`);
        logger.info(`ðŸ§  [THE BRAIN] LLM extracted keywords: [${llmKeywords.join(', ')}]`);
        
        // Loop through rules ONCE (first match wins by priority)
        for (let i = 0; i < triageRules.length; i++) {
            const rule = triageRules[i];
            
            // Check if this rule matches
            const matchResult = this.checkRuleMatch(input, llmKeywords, rule);
            
            if (matchResult.matched) {
                logger.info(`ðŸ§  [THE BRAIN] âœ… MATCH FOUND at index ${i}`, {
                    source: rule.source,
                    priority: rule.priority,
                    keywords: rule.keywords,
                    excludeKeywords: rule.excludeKeywords,
                    serviceType: rule.serviceType,
                    action: rule.action,
                    categorySlug: rule.categorySlug,
                    matchedKeywords: matchResult.matchedKeywords,
                    matchMethod: matchResult.matchMethod
                });
                
                // FIRST MATCH WINS - return immediately
                return {
                    source: rule.source,
                    priority: rule.priority,
                    keywords: rule.keywords,
                    excludeKeywords: rule.excludeKeywords,
                    serviceType: rule.serviceType,
                    action: rule.action,
                    categorySlug: rule.categorySlug,
                    explanation: rule.reason || rule.explanation || '',
                    matchedAt: new Date().toISOString(),
                    matchedKeywords: matchResult.matchedKeywords,
                    matchMethod: matchResult.matchMethod,
                    ruleIndex: i
                };
            }
        }
        
        // Should never reach here if fallback rule exists
        logger.error('ðŸ§  [THE BRAIN] âŒ No rule matched (fallback rule should catch everything!)');
        return null;
    }
    
    /**
     * Check if a single rule matches the input
     * @param {string} input - Normalized user input (lowercase)
     * @param {Array} llmKeywords - Keywords extracted by LLM
     * @param {Object} rule - Triage rule to check
     * @returns {Object} { matched: boolean, matchedKeywords: [], matchMethod: string }
     */
    static checkRuleMatch(input, llmKeywords, rule) {
        // Special case: Fallback rule (empty keywords = matches everything)
        if (rule.isFallback || (rule.keywords.length === 0 && rule.excludeKeywords.length === 0)) {
            return {
                matched: true,
                matchedKeywords: [],
                matchMethod: 'FALLBACK'
            };
        }
        
        const ruleKeywords = (rule.keywords || []).map(k => k.toLowerCase());
        const excludeKeywords = (rule.excludeKeywords || []).map(k => k.toLowerCase());
        
        // Step 1: Check if ALL required keywords are present (in input OR LLM keywords)
        const matchedKeywords = [];
        
        for (const keyword of ruleKeywords) {
            const inInput = input.includes(keyword);
            const inLLM = llmKeywords.some(lk => lk.includes(keyword) || keyword.includes(lk));
            
            if (inInput || inLLM) {
                matchedKeywords.push(keyword);
            } else {
                // Required keyword missing - no match
                return { matched: false, matchedKeywords: [], matchMethod: null };
            }
        }
        
        // Step 2: Check if any EXCLUDE keywords are present (in input OR LLM keywords)
        for (const excludeKeyword of excludeKeywords) {
            const inInput = input.includes(excludeKeyword);
            const inLLM = llmKeywords.some(lk => lk.includes(excludeKeyword) || excludeKeyword.includes(lk));
            
            if (inInput || inLLM) {
                // Exclude keyword found - no match
                return { matched: false, matchedKeywords: [], matchMethod: null };
            }
        }
        
        // All required keywords present, no exclude keywords found
        return {
            matched: true,
            matchedKeywords,
            matchMethod: 'KEYWORD_MATCH'
        };
    }
}

module.exports = FrontlineIntel;

